# 🛍 Taobao/Tmall 商品评论爬虫 (Selenium)

> 一个基于 Python 和 Selenium 的爬虫脚本，用于稳定、有效地从淘宝或天猫的商品详情页中抓取全部用户评论，并导出为 CSV 文件。本脚本特别针对处理登录状态下的动态加载（无限滚动）弹窗进行了优化。

## ✨ 主要特性

- **免登录/保持会话 (Cookie-based)**：通过注入本地保存的 Cookie 文件，实现免扫码登录，维持会话状态，避免频繁触发登录验证。
- **处理无限滚动 (Infinite Scroll)**：通过模拟滚动操作，触发评论弹窗中的懒加载机制，确保能够抓取所有历史评论。
- **优雅停止 (Keyboard Interrupt)**：支持在爬取过程中随时按下 `Ctrl + C`，程序会立即停止循环，并自动保存已抓取的所有评论数据到 CSV 文件，防止数据丢失。
- **精确数据提取**：精确提取买家昵称、评论内容和规范化的评论时间（仅保留年-月-日）。
- **模块化代码**：结构清晰，易于维护和配置。

## 🛠 环境要求

在运行此脚本之前，请确保你的环境中安装了以下组件：

1. **Python 3.x**
2. **Selenium 库**
3. **Chrome 浏览器**
4. **ChromeDriver**：确保你安装的 ChromeDriver 版本与你的 Chrome 浏览器版本兼容，并且已经将其路径添加到系统环境变量中（如果使用默认设置）。

### 安装依赖

使用 pip 安装所需的 Python 库：

```
pip install selenium
```

## ⚙️ 设置与使用

### 1. 配置 Cookie 文件

由于淘宝和天猫对未登录用户或爬虫的限制较高，本项目依赖于已登录用户的 Cookie 来保持会话。

1. **获取 Cookie**：在 Chrome 浏览器中，登录淘宝或天猫。使用浏览器扩展程序（例如：`EditThisCookie` 或类似工具）导出你的当前会话 Cookie。
2. **保存为 JSON**：将导出的 Cookie 数据保存为 JSON 格式的文件，并命名为 `cookie.json`，放置在项目根目录下。

> **注意**：Cookie 具有时效性。如果长时间未运行，需要重新导出新的 Cookie。

### 2. 配置脚本参数

打开 `你的脚本文件.py`，在 `main()` 函数顶部的 **`配置区域`** 修改以下三个变量：

| **变量名**    | **描述**                              | **示例值**                                   |
| ------------- | ------------------------------------- | -------------------------------------------- |
| `COOKIE_FILE` | Cookie 文件名                         | `"cookie.json"`                              |
| `PRODUCT_URL` | **必填**：需要爬取的淘宝/天猫商品链接 | `"https://detail.tmall.com/item.htm?id=..."` |
| `CSV_FILE`    | 导出结果的 CSV 文件名                 | `"taobao_comments.csv"`                      |

### 3. 运行脚本

在终端或命令行中，导航到项目目录，然后运行：

```shell
python requesy_comment.py
```

### 4. 手动停止 (Graceful Stop)

程序启动后，会打印提示信息：

💡提示：在终端按 Ctrl + C 可随时停止并保存数据

当你觉得抓取量足够时，直接按下 `Ctrl + C`，程序将自动捕获中断信号，并立即将当前已抓取的数据写入 CSV 文件，然后安全退出浏览器。

## ⚠️ 重要提示与免责声明

- **防爬策略**：淘宝/天猫的反爬机制可能会随时更新。如果脚本运行失败或被封禁，可能是网站前端结构或反爬机制发生变化，需要调整 XPath 或增加代理/验证码处理。
- **法律风险**：本工具仅供个人学习、研究和测试使用。请**严格遵守**相关网站的使用条款，不得将数据用于商业用途或侵犯他人隐私。使用本项目产生的一切后果由使用者自行承担。
- **XPath 稳定性**：脚本中用于定位评论弹窗和列表项的 XPath (`/html/body/div[7]/...`) 是基于特定时刻的网页结构编写的，可能因天猫页面更新而失效，届时需要手动更新对应的 XPath。
